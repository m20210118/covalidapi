{"cells":[{"cell_type":"code","source":["!pip install tensorflow\n","!pip install tensorflow_text"],"metadata":{"id":"2se76Xog6Bvr"},"id":"2se76Xog6Bvr","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"141e5b45","metadata":{"id":"141e5b45"},"outputs":[],"source":["import collections\n","import pathlib\n","import os\n","import tensorflow as tf\n","\n","from tensorflow.keras import layers\n","from tensorflow.keras import losses\n","from tensorflow.keras import utils\n","from tensorflow.keras.layers import TextVectorization\n","\n","import tensorflow_datasets as tfds\n","import tensorflow_text as tf_text"]},{"cell_type":"code","execution_count":null,"id":"4488b416","metadata":{"id":"4488b416"},"outputs":[],"source":["train_dir = pathlib.Path('D:\\\\Work\\\\WebApps\\\\Misinfo\\\\data\\\\train')\n","list(train_dir.iterdir())"]},{"cell_type":"code","execution_count":null,"id":"5c18211a","metadata":{"id":"5c18211a"},"outputs":[],"source":["test_dir = pathlib.Path('D:\\\\Work\\\\WebApps\\\\Misinfo\\\\data\\\\test')\n","list(test_dir.iterdir())"]},{"cell_type":"code","execution_count":null,"id":"84aced6d","metadata":{"id":"84aced6d"},"outputs":[],"source":["sample_file = train_dir/'fake/960090893.txt'\n","\n","with open(sample_file) as f:\n","  print(f.read())"]},{"cell_type":"code","execution_count":null,"id":"d3fb7a8f","metadata":{"id":"d3fb7a8f"},"outputs":[],"source":["batch_size = 32\n","seed = 42\n","\n","raw_train_ds = utils.text_dataset_from_directory(\n","    train_dir,\n","    batch_size=batch_size,\n","    validation_split=0.35,\n","    subset='training',\n","    seed=seed)"]},{"cell_type":"code","execution_count":null,"id":"4dad62e7","metadata":{"id":"4dad62e7"},"outputs":[],"source":["for text_batch, label_batch in raw_train_ds.take(1):\n","  for i in range(10):\n","    print(\"Tweet: \", text_batch.numpy()[i])\n","    print(\"Label:\", label_batch.numpy()[i])"]},{"cell_type":"code","execution_count":null,"id":"52ed8c68","metadata":{"id":"52ed8c68"},"outputs":[],"source":["for i, label in enumerate(raw_train_ds.class_names):\n","  print(\"Label\", i, \"corresponds to\", label)"]},{"cell_type":"code","execution_count":null,"id":"b7c62430","metadata":{"id":"b7c62430"},"outputs":[],"source":["# Create a validation set.\n","raw_val_ds = utils.text_dataset_from_directory(\n","    train_dir,\n","    batch_size=batch_size,\n","    validation_split=0.35,\n","    subset='validation',\n","    seed=seed)"]},{"cell_type":"code","execution_count":null,"id":"591d290b","metadata":{"id":"591d290b"},"outputs":[],"source":["# Create a test set.\n","raw_test_ds = utils.text_dataset_from_directory(\n","    test_dir,\n","    batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"id":"11e17c5c","metadata":{"id":"11e17c5c"},"outputs":[],"source":["VOCAB_SIZE = 10000\n","\n","binary_vectorize_layer = TextVectorization(\n","    max_tokens=VOCAB_SIZE,\n","    output_mode='binary')"]},{"cell_type":"code","execution_count":null,"id":"fa41b118","metadata":{"id":"fa41b118"},"outputs":[],"source":["VOCAB_SIZE = 10000\n","MAX_SEQUENCE_LENGTH = 250\n","\n","int_vectorize_layer = TextVectorization(\n","    max_tokens=VOCAB_SIZE,\n","    output_mode='int',\n","    output_sequence_length=MAX_SEQUENCE_LENGTH)"]},{"cell_type":"code","execution_count":null,"id":"5a84d998","metadata":{"id":"5a84d998"},"outputs":[],"source":["# Make a text-only dataset (without labels), then call `TextVectorization.adapt`.\n","train_text = raw_train_ds.map(lambda text, labels: text)\n","int_vectorize_layer.adapt(train_text)"]},{"cell_type":"code","execution_count":null,"id":"fb22d0c0","metadata":{"id":"fb22d0c0"},"outputs":[],"source":["def int_vectorize_text(text, label):\n","  text = tf.expand_dims(text, -1)\n","  return int_vectorize_layer(text), label"]},{"cell_type":"code","execution_count":null,"id":"5478866d","metadata":{"id":"5478866d"},"outputs":[],"source":["# Retrieve a batch (of 32 tweets and labels) from the dataset.\n","text_batch, label_batch = next(iter(raw_train_ds))\n","first_question, first_label = text_batch[0], label_batch[0]\n","print(\"Tweet\", first_question)\n","print(\"Label\", first_label)"]},{"cell_type":"code","execution_count":null,"id":"e16c976a","metadata":{"id":"e16c976a"},"outputs":[],"source":["int_train_ds = raw_train_ds.map(int_vectorize_text)\n","int_val_ds = raw_val_ds.map(int_vectorize_text)\n","int_test_ds = raw_test_ds.map(int_vectorize_text)"]},{"cell_type":"code","execution_count":null,"id":"dd7fab28","metadata":{"id":"dd7fab28"},"outputs":[],"source":["AUTOTUNE = tf.data.AUTOTUNE\n","\n","def configure_dataset(dataset):\n","  return dataset.cache().prefetch(buffer_size=AUTOTUNE)"]},{"cell_type":"code","execution_count":null,"id":"273fe7e6","metadata":{"id":"273fe7e6"},"outputs":[],"source":["int_train_ds = configure_dataset(int_train_ds)\n","int_val_ds = configure_dataset(int_val_ds)\n","int_test_ds = configure_dataset(int_test_ds)"]},{"cell_type":"code","execution_count":null,"id":"b28dd8cf","metadata":{"id":"b28dd8cf"},"outputs":[],"source":["def create_model(vocab_size, num_labels):\n","  model = tf.keras.Sequential([\n","      layers.Embedding(vocab_size, 64, mask_zero=True),\n","      layers.Conv1D(64, 5, padding=\"valid\", activation=\"relu\", strides=2),\n","      layers.GlobalMaxPooling1D(),\n","      layers.Dense(num_labels)\n","  ])\n","  return model"]},{"cell_type":"code","execution_count":null,"id":"a18fea76","metadata":{"id":"a18fea76"},"outputs":[],"source":["# `vocab_size` is `VOCAB_SIZE + 1` since `0` is used additionally for padding.\n","int_model = create_model(vocab_size=VOCAB_SIZE + 1, num_labels=4)\n","int_model.compile(\n","    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n","    optimizer='adam',\n","    metrics=['accuracy'])\n","history = int_model.fit(int_train_ds, validation_data=int_val_ds, epochs=5)"]},{"cell_type":"code","execution_count":null,"id":"23970a8c","metadata":{"id":"23970a8c"},"outputs":[],"source":["int_loss, int_accuracy = int_model.evaluate(int_test_ds)\n","\n","print(\"Int model accuracy: {:2.2%}\".format(int_accuracy))"]},{"cell_type":"code","execution_count":null,"id":"8c37792c","metadata":{"id":"8c37792c"},"outputs":[],"source":["# Make a text-only dataset (without labels), then call `TextVectorization.adapt`.\n","train_text = raw_train_ds.map(lambda text, labels: text)\n","binary_vectorize_layer.adapt(train_text)"]},{"cell_type":"code","execution_count":null,"id":"a1453206","metadata":{"id":"a1453206"},"outputs":[],"source":["def binary_vectorize_text(text, label):\n","  text = tf.expand_dims(text, -1)\n","  return binary_vectorize_layer(text), label"]},{"cell_type":"code","execution_count":null,"id":"60ad812d","metadata":{"id":"60ad812d"},"outputs":[],"source":["# Retrieve a batch (of 32 tweets and labels) from the dataset.\n","text_batch, label_batch = next(iter(raw_train_ds))\n","first_question, first_label = text_batch[0], label_batch[0]\n","print(\"Tweet\", first_question)\n","print(\"Label\", first_label)"]},{"cell_type":"code","execution_count":null,"id":"26f93e88","metadata":{"id":"26f93e88"},"outputs":[],"source":["binary_train_ds = raw_train_ds.map(binary_vectorize_text)\n","binary_val_ds = raw_val_ds.map(binary_vectorize_text)\n","binary_test_ds = raw_test_ds.map(binary_vectorize_text)"]},{"cell_type":"code","execution_count":null,"id":"e26c6db3","metadata":{"id":"e26c6db3"},"outputs":[],"source":["binary_train_ds = configure_dataset(binary_train_ds)\n","binary_val_ds = configure_dataset(binary_val_ds)\n","binary_test_ds = configure_dataset(binary_test_ds)"]},{"cell_type":"code","execution_count":null,"id":"8643a213","metadata":{"scrolled":true,"id":"8643a213"},"outputs":[],"source":["binary_model = tf.keras.Sequential([layers.Dense(4)])\n","\n","binary_model.compile(\n","    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n","    optimizer='adam',\n","    metrics=['accuracy'])\n","\n","history = binary_model.fit(\n","    binary_train_ds, validation_data=binary_val_ds, epochs=10)"]},{"cell_type":"code","execution_count":null,"id":"3429cd3c","metadata":{"id":"3429cd3c"},"outputs":[],"source":["binary_loss, binary_accuracy = binary_model.evaluate(binary_test_ds)\n","\n","print(\"Binary model accuracy: {:2.2%}\".format(binary_accuracy))"]},{"cell_type":"code","execution_count":null,"id":"6eefe5e0","metadata":{"id":"6eefe5e0"},"outputs":[],"source":["export_model = tf.keras.Sequential(\n","    [binary_vectorize_layer, binary_model,\n","     layers.Activation('swish')])\n","\n","export_model.compile(\n","    loss=losses.SparseCategoricalCrossentropy(from_logits=False),\n","    optimizer='adam',\n","    metrics=['accuracy'])\n","\n","# Test it with `raw_test_ds`, which yields raw strings\n","loss, accuracy = export_model.evaluate(raw_test_ds)\n","print(\"Accuracy: {:2.2%}\".format(binary_accuracy))"]},{"cell_type":"code","execution_count":null,"id":"31c418d5","metadata":{"id":"31c418d5"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"id":"b4b0cb99","metadata":{"id":"b4b0cb99"},"outputs":[],"source":["def get_string_labels(predicted_scores_batch):\n","  predicted_int_labels = tf.argmax(predicted_scores_batch, axis=1)\n","  predicted_labels = tf.gather(raw_train_ds.class_names, predicted_int_labels)\n","  return predicted_labels"]},{"cell_type":"code","execution_count":null,"id":"31696218","metadata":{"id":"31696218"},"outputs":[],"source":["inputs = [\n","    \"Covid causes cancer.\"\n","]\n","predicted_scores = export_model.predict(inputs)\n","predicted_labels = get_string_labels(predicted_scores)\n","for input, label in zip(inputs, predicted_labels):\n","  print(\"Tweet: \", input)\n","  print(\"Predicted label: \", label.numpy())"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"name":"2. tf_word_embed.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":5}